# 2022-NLP-PR
This is BIT NLP experiments calss final homework paper reproduction.
This repository contains data and code for ACL 2021 paper 

					COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic
					Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan.

Before coding i trying to figure out the purpose of this paper and method of experiment.
There is plenty of information on internet but due to indiscriminate provision of information, medical-information cannot be 
distinguished accurately. That's why we need tool of check the fact automatically.
The pipe line constructed by 
 1) consider real-world claim
 2) Retrieve relevant documents not bounded to a known document collection and which contain information to validate the claim.
 3) Select evidence sentences that can support of refute claim.
 4) Predict the claim veracity based on this evidence.



# COVID-Fact Dataset 
There is 4086 real-world claims with corresponding evidence documents and evidence sentences to support of refute the claim.
There are 1296 supported claims and 2790 automatically generated refuted claims.
And they semi-automatically construct the dataset.
1) select real-world true claims and trustworthy evidence documents.
2) automatic counter-claim generation
3) evidence sentence seleciton.

# Evidence selection.
evidence selection and select evidence for a claim can be found in the evidence_sel folder.
Evidence is scraped form top 5 Google Search results for the claim, including the source of the claim link.
List of evidence sentences is retrieved by first creating a "corpus" by scraping top 5 google search results for the claim, and 
then retrieving the most similar sentences to the claim using SBERT.

# Counter claim Generation
Counter claim generation can be found in the generate folder. get_attn_model.py is used to find the best head and layer of a
fine tuned BERT uncased model. get_top_words.py is used to obtain the salient words for a particular set of claims. Finally, gen_contrast_claims.py is used to replace salient words by outputs generated by RoBERTA fine-tuned on CORD-19.

# Evaluation
eval_data folder provides the dataset already processed in the format necessary for the experiments. The train/test splits are contained in the folders with prefix RTE.Run result.py for accuracy F1 score .

# Conclusion
In this paper a dataset of 4086 claims concerning the COVID-19 pandemic, together with supporting and refuting evidence. This
experiments reveal that our dataset outperforms zero-shot baselines trained on pupular fact checking benchemarks. finally provide a detailed evaluation of the covid fact task.


